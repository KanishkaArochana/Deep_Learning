# ðŸ§  Neural Networks & Activation Functions â€“ Documentation

## ðŸ“Œ 1. What is a Neuron?
A **neuron** is the fundamental building block of a neural network. It is a computational unit inspired by biological neurons. It:

- Takes one or more input values.
- Applies weights and bias to inputs.
- Passes the result through an **activation function**.
- Produces an output.

**Mathematically:**  
`Output = ActivationFunction(WÂ·X + b)`

Where:
- **W** = weights  
- **X** = input  
- **b** = bias

---

## ðŸ“Œ 2. What is a Neural Network?
A **Neural Network** is a collection of interconnected **artificial neurons** arranged in layers:

- **Input Layer**: Takes raw data as input.
- **Hidden Layers**: Perform computations and feature extraction.
- **Output Layer**: Produces the final prediction.

Neural networks are used in tasks like image recognition, natural language processing, and classification problems.

---

## ðŸ“Œ 3. What is an Activation Function?
An **activation function** defines the output of a neuron given an input or set of inputs. It introduces **non-linearity** into the model, allowing the network to learn complex patterns.

---

## âš™ï¸ 4. Common Artificial Neuron Activation Functions

### ðŸ”¹ 1. Sigmoid Function
- **Formula:**  
  `f(x) = 1 / (1 + e^(-x))`
- **Range:** (0, 1)
- **Use Case:** Binary classification (2 classes)
- **Examples:**
  - Spam vs. Not Spam  
  - Rain vs. No Rain  
  - Dog vs. Cat  

âœ… Smooth gradient  

âœ… Graph:

![Sigmoid Graph](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)


---

### ðŸ”¹ 2. Tanh (Hyperbolic Tangent) Function
- **Formula:**  
  `f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))`
- **Range:** (-1, +1)
- **Use Case:** Situations where outputs can be positive or negative
- **Advantage:** Stronger gradients than Sigmoid for negative inputs

âœ… Centered at zero 

âœ… Graph: 

![Tanh Graph](https://upload.wikimedia.org/wikipedia/commons/c/cb/Activation_tanh.svg)

---

### ðŸ”¹ 3. ReLU (Rectified Linear Unit)
- **Formula:**  
  `f(x) = max(0, x)`
- **Range:** [0, âˆž)
- **Use Case:** Default in most deep learning models (fast and simple)

âœ… Efficient and widely used  

âœ… Graph:

![ReLU Graph](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)


---

### ðŸ”¸ Leaky ReLU (Solution to Dying ReLU)
- **Formula:**  
  ```
  f(x) = {
      x      if x > 0
      Î±Â·x    if x â‰¤ 0
  }
  ```
  Typically: Î± = 0.01

- Allows a small gradient when `x < 0`

âœ… Helps avoid dying ReLU  
âœ… Retains simplicity of ReLU


âœ… Graph:
![Leaky ReLU Graph](https://upload.wikimedia.org/wikipedia/commons/f/fe/Activation_prelu.svg)


---

### ðŸ”¹ 4. Softmax Function
- **Formula:**  
  `Softmax(z_i) = e^(z_i) / Î£ e^(z_j)`
- **Range:** (0, 1), and all outputs sum to 1
- **Use Case:** Multi-class classification problems
- **Examples:**
  - Classifying among Dog, Cat, Rabbit
  - Digit recognition (0â€“9)

âœ… Converts outputs into probabilities  
âœ… Good for final layer in multi-class models


âœ… Graph:

![Softmax Graph](https://upload.wikimedia.org/wikipedia/commons/5/5f/Softmax-function.svg)



---

## âœ… Summary Table

| Activation Function | Output Range | Use Case                    | Issues               |
|---------------------|--------------|-----------------------------|----------------------|
| Sigmoid             | (0, 1)       | Binary classification       | Vanishing gradient   |
| Tanh                | (-1, 1)      | Positive/negative outputs   | Vanishing gradient   |
| ReLU                | [0, âˆž)       | Default for hidden layers   | Dying ReLU           |
| Leaky ReLU          | (-âˆž, âˆž)      | Fix for ReLU issue          | Slight complexity    |
| Softmax             | (0, 1), sum=1| Multi-class classification  | Slower, soft output  |